---
title: "Why Not Transform Chat Large Language Models to Non-English?"
collection: publications
category: preprint
permalink: /publication/2025-02-transLLM
date: 2025-02-01
venue: 'Preprint'
excerpt: 'Chat large language models (LLMs), fine-tuned from pre-trained models and optimized for alignment with human preferences, excel in following diverse instructions while maintaining consistency with human values. In this paper, we propose the TransLLM framework for transforming chat LLMs from English to other languages using publicly available resources. TransLLM employs the translation chain-of-thought (TCoT) technique, which transfers chat ability through inference-time computation. Specifically, for a query in the target language, TCoT guides the LLM to first generate an English query and response as intermediate transfer steps before producing the final response in the target language. We underscore the necessity of improving the performance of each step in TCoT. However, improvement through continual pre-training (CPT) induces catastrophic forgetting of the original chat ability. To address this issue, we introduce recovery knowledge distillation (RKD), which utilizes data generated by the original chat LLM to recover its chat ability. Experimental results indicate that TransLLM outperforms baseline models across various languages and LLMs while demonstrating adaptability in multilingual settings and generalizability beyond its training tasks. Our analysis elucidates the mechanism by which RKD, in conjunction with LoRA, mitigates catastrophic forgetting.'
paperurl: 'https://arxiv.org/pdf/2405.13923'
bibtexurl: '/files/2025-02-transLLM.bib'
citation: 'Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Xinglin Lyu, Min Zhang, Jiajun Chen, Hao Yang, and Shujian Huang. 2024. Why Not Transform Chat Large Language Models to Non-English? <i>arXiv preprint arXiv:2405.13923</i>.'
---