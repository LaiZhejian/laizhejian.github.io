{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Publications markdown generator for academicpages\n",
    "\n",
    "Takes a TSV of publications with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). The core python code is also in `publications.py`. Run either from the `markdown_generator` folder after replacing `publications.tsv` with one containing your data.\n",
    "\n",
    "TODO: Make this work with BibTex and other databases of citations, rather than Stuart's non-standard TSV format and citation style.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data format\n",
    "\n",
    "The TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top. \n",
    "\n",
    "- `excerpt` and `paper_url` can be blank, but the others must have values. \n",
    "- `pub_date` must be formatted as YYYY-MM-DD.\n",
    "- `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]`\n",
    "\n",
    "This is how the raw file looks (it doesn't look pretty, use a spreadsheet or other program to edit and create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_slug\tpub_date\ttitle\tcategory\tvenue\texcerpt\tcitation\tpaper_url\tslides_url\n",
      "alignment_analysis\t2025-05-15\tHow does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective\tpreprint\tPreprint\tMultilingual Alignment is an effective and representative paradigm to enhance LLMs’ multilingual capabilities, which transfers the capabilities from the high-resource languages to the low-resource languages. Meanwhile, some researches on language-specific neurons reveal that there are language-specific neurons that are selectively activated in LLMs when processing different languages. This provides a new perspective to analyze and understand LLMs’ mechanisms more specifically in multilingual scenarios. In this work, we propose a new finer-grained neuron identification algorithm, which detects language neurons (including language-specific neurons and language-related neurons) and language-agnostic neurons. Furthermore, based on the distributional characteristics of different types of neurons, we divide the LLMs’ internal process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting. Additionally, we systematically analyze the models before and after alignment with a focus on different types of neurons. We also analyze the phenomenon of “Spontaneous Multilingual Alignment”. Overall, our work conducts a comprehensive investigation based on different types of neurons, providing empirical results and valuable insights for better understanding multilingual alignment and multilingual capabilities of LLMs.\tShimao Zhang, Zhejian Lai, Xiang Liu, Shuaijie She, Xiao Liu, Yeyun Gong, Shujian Huang and Jiajun Chen. 2025. How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective. <i>arXiv preprint arXiv:2502.21505</i>.\thttps://arxiv.org/pdf/2505.21505\t\n",
      "DCSQE\t2025-02-01\tAlleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation\tconference\tACL Long Paper\tQuality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce DCSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. DCSQE uses references—i.e., translation supervision signals—to guide both the generation and annotation processes, enhancing the quality of token-level labels. DCSQE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks.\tXiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang, and Shujian Huang. 2025. Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation. <i>arXiv preprint arXiv:2502.19941</i>.\thttps://arxiv.org/pdf/2502.19941\t\n",
      "transLLM\t2025-02-01\tWhy Not Transform Chat Large Language Models to Non-English?\tpreprint    preprint\tPreprint\tChat large language models (LLMs), fine-tuned from pre-trained models and optimized for alignment with human preferences, excel in following diverse instructions while maintaining consistency with human values. In this paper, we propose the TransLLM framework for transforming chat LLMs from English to other languages using publicly available resources. TransLLM employs the translation chain-of-thought (TCoT) technique, which transfers chat ability through inference-time computation. Specifically, for a query in the target language, TCoT guides the LLM to first generate an English query and response as intermediate transfer steps before producing the final response in the target language. We underscore the necessity of improving the performance of each step in TCoT. However, improvement through continual pre-training (CPT) induces catastrophic forgetting of the original chat ability. To address this issue, we introduce recovery knowledge distillation (RKD), which utilizes data generated by the original chat LLM to recover its chat ability. Experimental results indicate that TransLLM outperforms baseline models across various languages and LLMs while demonstrating adaptability in multilingual settings and generalizability beyond its training tasks. Our analysis elucidates the mechanism by which RKD, in conjunction with LoRA, mitigates catastrophic forgetting.\tXiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai, Wei Zou, Shuaijie She, Jiaxin Guo, Xiaofeng Zhao, Yinglu Li, Yuang Li, Chang Su, Yanqing Zhao, Xinglin Lyu, Min Zhang, Jiajun Chen, Hao Yang, and Shujian Huang. 2024. Why Not Transform Chat Large Language Models to Non-English? <i>arXiv preprint arXiv:2405.13923</i>.\thttps://arxiv.org/pdf/2405.13923\t\n",
      "MQMQE\t2023-12-01\tUnify word-level and span-level tasks: NJUNLP’s Participation for the WMT2023 Quality Estimation Shared Task\tworkshop\tWMT2023 Shared Task\tWe introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework. We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considerable margin.\tXiang Geng, Zhejian Lai, Yu Zhang, Shimin Tao, Hao Yang, Jiajun Chen, and Shujian Huang. 2023. Unify Word-level and Span-level Tasks: NJUNLP’s Participation for the WMT2023 Quality Estimation Shared Task. <i>In Proceedings of the Eighth Conference on Machine Translation</i>, pages 829–834, Singapore. Association for Computational Linguistics.\thttps://arxiv.org/pdf/2309.13230\t\n",
      "CBSQE\t2023-12-01\tImproved pseudo data for machine translation quality estimation with constrained beam search\tconference\tEMNLP Long Paper\tMachine translation(MT) quality estimation(QE) is a crucial task to estimate the quality of MT outputs when reference translations are unavailable. Many studies focus on generating pseudo data using large parallel corpus and achieve remarkable success in the supervised setting. However, pseudo data solutions are less satisfying in unsupervised scenarios because the pseudo labels are inaccurate or the pseudo translations differ from the real ones. To address these problems, we propose to generate pseudo data using the MT model with constrained beam search~(CBSQE). CBSQE preserves the reference parts with high MT probabilities as correct translations, while the rest parts as the wrong ones for MT generation. Therefore, CBSQE can reduce the false negative labels caused by synonyms. Overall, beam search will prefer a more real hypothesis with a higher MT generation likelihood. Extensive experiments demonstrate that CBSQE outperforms strong baselines in both supervised and unsupervised settings. Analyses further show the superiority of CBSQE.\tXiang Geng, Yu Zhang, Zhejian Lai, Shuaijie She, Wei Zou, Shimin Tao, Hao Yang, Jiajun Chen, and Shujian Huang. 2023. Improved Pseudo Data for Machine Translation Quality Estimation with Constrained Beam Search. <i>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</i>, pages 12434–12447, Singapore. Association for Computational Linguistics.\thttps://aclanthology.org/2023.emnlp-main.764.pdf\t"
     ]
    }
   ],
   "source": [
    "!cat publications.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pandas\n",
    "\n",
    "We are using the very handy pandas library for dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import TSV\n",
    "\n",
    "Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n",
    "\n",
    "I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_slug</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>venue</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>citation</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>slides_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alignment_analysis</td>\n",
       "      <td>2025-05-15</td>\n",
       "      <td>How does Alignment Enhance LLMs' Multilingual ...</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Preprint</td>\n",
       "      <td>Multilingual Alignment is an effective and rep...</td>\n",
       "      <td>Shimao Zhang, Zhejian Lai, Xiang Liu, Shuaijie...</td>\n",
       "      <td>https://arxiv.org/pdf/2505.21505</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DCSQE</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>Alleviating Distribution Shift in Synthetic Da...</td>\n",
       "      <td>conference</td>\n",
       "      <td>ACL Long Paper</td>\n",
       "      <td>Quality Estimation (QE) models evaluate the qu...</td>\n",
       "      <td>Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang...</td>\n",
       "      <td>https://arxiv.org/pdf/2502.19941</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transLLM</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>Why Not Transform Chat Large Language Models t...</td>\n",
       "      <td>preprint    preprint</td>\n",
       "      <td>Preprint</td>\n",
       "      <td>Chat large language models (LLMs), fine-tuned ...</td>\n",
       "      <td>Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai,...</td>\n",
       "      <td>https://arxiv.org/pdf/2405.13923</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MQMQE</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Unify word-level and span-level tasks: NJUNLP’...</td>\n",
       "      <td>workshop</td>\n",
       "      <td>WMT2023 Shared Task</td>\n",
       "      <td>We introduce the submissions of the NJUNLP tea...</td>\n",
       "      <td>Xiang Geng, Zhejian Lai, Yu Zhang, Shimin Tao,...</td>\n",
       "      <td>https://arxiv.org/pdf/2309.13230</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBSQE</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>Improved pseudo data for machine translation q...</td>\n",
       "      <td>conference</td>\n",
       "      <td>EMNLP Long Paper</td>\n",
       "      <td>Machine translation(MT) quality estimation(QE)...</td>\n",
       "      <td>Xiang Geng, Yu Zhang, Zhejian Lai, Shuaijie Sh...</td>\n",
       "      <td>https://aclanthology.org/2023.emnlp-main.764.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             url_slug    pub_date  \\\n",
       "0  alignment_analysis  2025-05-15   \n",
       "1               DCSQE  2025-02-01   \n",
       "2            transLLM  2025-02-01   \n",
       "3               MQMQE  2023-12-01   \n",
       "4               CBSQE  2023-12-01   \n",
       "\n",
       "                                               title              category  \\\n",
       "0  How does Alignment Enhance LLMs' Multilingual ...              preprint   \n",
       "1  Alleviating Distribution Shift in Synthetic Da...            conference   \n",
       "2  Why Not Transform Chat Large Language Models t...  preprint    preprint   \n",
       "3  Unify word-level and span-level tasks: NJUNLP’...              workshop   \n",
       "4  Improved pseudo data for machine translation q...            conference   \n",
       "\n",
       "                 venue                                            excerpt  \\\n",
       "0             Preprint  Multilingual Alignment is an effective and rep...   \n",
       "1       ACL Long Paper  Quality Estimation (QE) models evaluate the qu...   \n",
       "2             Preprint  Chat large language models (LLMs), fine-tuned ...   \n",
       "3  WMT2023 Shared Task  We introduce the submissions of the NJUNLP tea...   \n",
       "4     EMNLP Long Paper  Machine translation(MT) quality estimation(QE)...   \n",
       "\n",
       "                                            citation  \\\n",
       "0  Shimao Zhang, Zhejian Lai, Xiang Liu, Shuaijie...   \n",
       "1  Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang...   \n",
       "2  Xiang Geng, Ming Zhu, Jiahuan Li, Zhejian Lai,...   \n",
       "3  Xiang Geng, Zhejian Lai, Yu Zhang, Shimin Tao,...   \n",
       "4  Xiang Geng, Yu Zhang, Zhejian Lai, Shuaijie Sh...   \n",
       "\n",
       "                                          paper_url  slides_url  \n",
       "0                  https://arxiv.org/pdf/2505.21505         NaN  \n",
       "1                  https://arxiv.org/pdf/2502.19941         NaN  \n",
       "2                  https://arxiv.org/pdf/2405.13923         NaN  \n",
       "3                  https://arxiv.org/pdf/2309.13230         NaN  \n",
       "4  https://aclanthology.org/2023.emnlp-main.764.pdf         NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications = pd.read_csv(\"publications.tsv\", sep=\"\\t\", header=0)\n",
    "publications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escape special characters\n",
    "\n",
    "YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "html_escape_table = {\n",
    "    \"&\": \"&amp;\",\n",
    "    '\"': \"&quot;\",\n",
    "    \"'\": \"&apos;\"\n",
    "    }\n",
    "\n",
    "def html_escape(text):\n",
    "    \"\"\"Produce entities within text.\"\"\"\n",
    "    return \"\".join(html_escape_table.get(c,c) for c in text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the markdown files\n",
    "\n",
    "This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "for row, item in publications.iterrows():\n",
    "    \n",
    "    md_filename = str(item.pub_date[:7]) + \"-\" + item.url_slug + \".md\"\n",
    "    html_filename = str(item.pub_date[:7]) + \"-\" + item.url_slug\n",
    "    year = item.pub_date[:4]\n",
    "    \n",
    "    ## YAML variables\n",
    "    \n",
    "    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n",
    "    \n",
    "    md += \"\"\"collection: publications\"\"\"\n",
    "    \n",
    "    md += f\"\"\"\\ncategory: {item.category}\"\"\"\n",
    "    \n",
    "    md += \"\"\"\\npermalink: /publication/\"\"\" + html_filename\n",
    "    \n",
    "    md += \"\\ndate: \" + str(item.pub_date) \n",
    "    \n",
    "    md += \"\\nvenue: '\" + html_escape(item.venue) + \"'\"\n",
    "    \n",
    "    \n",
    "    if len(str(item.excerpt)) > 5:\n",
    "        md += \"\\nexcerpt: '\" + html_escape(item.excerpt) + \"'\"\n",
    "    \n",
    "    if len(str(item.slides_url)) > 5:\n",
    "        md += \"\\nslidesurl: '\" + item.slides_url + \"'\"\n",
    "\n",
    "    if len(str(item.paper_url)) > 5:\n",
    "        md += \"\\npaperurl: '\" + item.paper_url + \"'\"\n",
    "        \n",
    "    md += \"\\nbibtexurl: '/files/\" + html_filename + \".bib'\"\n",
    "        \n",
    "    md += \"\\ncitation: '\" + html_escape(item.citation) + \"'\"\n",
    "    \n",
    "    md += \"\\n---\"\n",
    "    \n",
    "    ## Markdown description for individual page\n",
    "        \n",
    "    # if len(str(item.excerpt)) > 5:\n",
    "    #     md += \"\\n\" + html_escape(item.excerpt) + \"\\n\"\n",
    "\n",
    "    # if len(str(item.slides_url)) > 5:\n",
    "    #     md += \"\\n[Download slides here](\" + item.slides_url + \")\\n\" \n",
    "\n",
    "    # if len(str(item.paper_url)) > 5:\n",
    "    #     md += \"\\n[Download paper here](\" + item.paper_url + \")\\n\" \n",
    "        \n",
    "    # md += \"\\nRecommended citation: \" + item.citation\n",
    "    \n",
    "    md_filename = os.path.basename(md_filename)\n",
    "       \n",
    "    with open(\"../_publications/\" + md_filename, 'w') as f:\n",
    "        f.write(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in the publications directory, one directory below where we're working from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine translation(MT) quality estimation(QE) is a crucial task to estimate the quality of MT outputs when reference translations are unavailable. Many studies focus on generating pseudo data using large parallel corpus and achieve remarkable success in the supervised setting. However, pseudo data solutions are less satisfying in unsupervised scenarios because the pseudo labels are inaccurate or the pseudo translations differ from the real ones. To address these problems, we propose to generate pseudo data using the MT model with constrained beam search~(CBSQE). CBSQE preserves the reference parts with high MT probabilities as correct translations, while the rest parts as the wrong ones for MT generation. Therefore, CBSQE can reduce the false negative labels caused by synonyms. Overall, beam search will prefer a more real hypothesis with a higher MT generation likelihood. Extensive experiments demonstrate that CBSQE outperforms strong baselines in both supervised and unsupervised settings. Analyses further show the superiority of CBSQE.'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_escape(item.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-CBSQE.md              2025-02-transLLM.md\n",
      "2023-12-MQMQE.md              2025-05-alignment_analysis.md\n",
      "2025-02-DCSQE.md\n"
     ]
    }
   ],
   "source": [
    "!ls ../_publications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: ../_publications/2009-10-01-paper-title-number-1.md: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cat ../_publications/2009-10-01-paper-title-number-1.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
